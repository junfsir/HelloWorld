```shell
sysdig：
perf：
```

---

**[内核动态追踪技术](https://openresty.org/posts/dynamic-tracing/)**

[bcc](https://github.com/iovisor/bcc)

[perf-tools](https://github.com/brendangregg/perf-tools)

---

**[debugfs]( http://tinylab.org/show-the-usage-of-procfs-sysfs-debugfs/ )**

Linux一个专门为调试目的而设计的基于RAM的文件系统；

内核中有三个常用的伪文件系统：procfs，debugfs和sysfs；

procfs — The proc filesystem is a pseudo-filesystem which provides an interface to kernel data structures.
sysfs — The filesystem for exporting kernel objects.
debugfs — Debugfs exists as a simple way for kernel developers to make information available to user space.
它们都用于Linux内核和用户空间的数据交换，但是适用的场景有所差异：

- procfs 历史最早，最初就是用来跟内核交互的唯一方式，用来获取处理器、内存、设备驱动、进程等各种信息；
- sysfs 跟 kobject 框架紧密联系，而 kobject 是为设备驱动模型而存在的，所以 sysfs 是为设备驱动服务的；
- debugfs 从名字来看就是为debug而生，所以更加灵活；

---

**[uprobes](http://www.brendangregg.com/blog/2015-06-28/linux-ftrace-uprobe.html)**

user-level dynamic tracing

---

**[kprobes](https://lwn.net/Articles/132196/)**

---

**[ftrace]( https://linux.cn/article-9273-1.html)**

命令行工具：trace-cmd

 ftrace 是一个 Linux 内核特性，它可以让你去跟踪 Linux 内核的函数调用；

 https://www.ibm.com/developerworks/cn/linux/l-cn-ftrace1/index.html 

 https://www.ibm.com/developerworks/cn/linux/l-cn-ftrace2/index.html 

 https://www.ibm.com/developerworks/cn/linux/l-cn-ftrace3/index.html 

---

**[kdump]( https://zh.wikipedia.org/wiki/Kdump)**

kdump是Linux内核的一个功能，可在发生内核错误时创建核心转储；当被触发时，kdump会导出一个内存映像（也称为vmcore），该映像可用于调试和确定崩溃的原因。 主内存的转储映像作为可执行与可链接格式（ELF）对象导出，可以在处理内核崩溃时通过/proc/vmcore直接访问，也可以自动保存到本地可访问的文件系统、 裸设备或通过网络访问的远程系统；

核心转储（core dump），是操作系统在进程收到某些信号而终止运行时，将此时进程地址空间的内容以及有关进程状态的其他信息写出的一个磁盘文件；

---

对于32位的Linux系统，CPU能访问4GB的虚拟地址空间0x0～0xFFFFFFFF。如图所示，其中低3GB的地址（0x0～0xC0000000）是应用层的地址空间，高地址的1GB （0xC0000000～0xFFFFFFFF）是留给kernel的；

kernel里所有的线程是共享那1GB的地址空间，而每个应用进程都是有独立私有的3GB地址空间，互相之间不干扰；

![](https://github.com/junfsir/jNote/raw/master/images/linux内存地址空间.png)

---

 在Linux下，文件的缓存习惯性称之为`Page Cache`，而更低一级的设备的缓存称之为`Buffer Cache`. 这两个概念很容易混淆，这里简单的介绍下概念上的区别：`Page Cache`用于缓存文件的内容，和文件系统比较相关。文件的内容需要映射到实际的物理磁盘，这种映射关系由文件系统来完成；`Buffer Cache`用于缓存存储设备块（比如磁盘扇区）的数据，而不关心是否有文件系统的存在（文件系统的元数据缓存在`Buffer Cache`中）。 

---

kswapd

这种内核线程只有一个，主要作用是用来回收内存。在kswapd中，有2个阀值，pages_hige和pages_low。当空闲内存页的数量低于pages_low的时候，kswapd进程就会扫描内存并且每次释放出32个 free pages，直到freepage的数量到达pages_high。具体回收内存有如下原则：

1. 如果页未经更改就将该页放入空闲队列；
2. 如果页已经更改并且是可备份回文件系统的，就理解将内存页的内容写回磁盘；
3. 如果页已经更改但是没有任何磁盘上的备份，就将其写入swap分区。
       同样，属于核心的内存管理线程，这个线程也不能被关闭

---

migration

每个处理器核对应一个migration内核线程，主要作用是作为相应CPU核的迁移进程，用来执行进程迁移操作，内核中的函数是migration_thread()。属于2.6内核的负载平衡系统，该进程在系统启动时自动加载（每个 cpu 一个），并将自己设为 SCHED_FIFO 的实时进程，然后检查 runqueue::migration_queue 中是否有请求等待处理，如果没有，就在 TASK_INTERRUPTIBLE 中休眠，直至被唤醒后再次检查。migration_queue仅在set_cpu_allowed() 中添加，当进程（比如通过 APM 关闭某 CPU 时）调用set_cpu_allowed()改变当前可用 cpu，从而使某进程不适于继续在当前 cpu 上运行时，就会构造一个迁移请求数据结构 migration_req_t，将其植入进程所在 cpu 就绪队列的migration_queue 中，然后唤醒该就绪队列的迁移 daemon（记录在runqueue::migration_thread 属性中），将该进程迁移到合适的cpu上去在目前的实现中，目的 cpu 的选择和负载无关，而是"any_online_cpu(req->task->cpus_allowed)"，也就是按 CPU 编号顺序的第一个 allowed 的CPU。所以，和 load_balance() 与调度器、负载平衡策略密切相关不同，migration_thread() 应该说仅仅是一个 CPU 绑定以及 CPU 电源管理等功能的一个接口。这个线程是调度系统的重要组成部分，也不能被关闭。

---

[idle进程](https://www.cnblogs.com/lp1129/p/3421440.html)

pid为0，是系统创建的第一个进程，也是唯一一个没有通过fork()产生的进程；在smp系统中，每个处理器单元有独立的一个运行队列，而每个运行队列上又有一个idle进程，即有多少处理器单元，就有多少idle进程；系统的空闲时间，其实就是指idle进程的“运行时间”；

idle 进程优先级为MAX_PRIO，即最低优先级；早先版本中，idle是参与调度的，所以将其优先级设为最低，当没有其他进程可以运行时，才会调度执行 idle；

而目前的版本中idle并不在运行队列中参与调度，而是在运行队列结构中含idle指针，指向idle进程，在调度器发现运行队列为空的时候运行，调入运行；

*小结:*
　　*1.idle是一个进程，其pid为0；*
　　*2.主处理器上的idle由原始进程(pid=0)演变而来。从处理器上的idle由init进程fork得到，但是它们的pid都为0；*
　　*3.Idle进程为最低优先级，且不参与调度，只是在运行队列为空的时候才被调度；*
　　*4.Idle循环等待need_resched置位。默认使用hlt节能；*

---

cgroup内存泄漏

https://www.jianshu.com/p/033fe2518476

http://www.iceyao.com.cn/2020/01/04/记一次k8s-cgroup内存泄露问题解决/

问题先从Linux的内存管理机制说起。在Linux中，内存分为内核内存及用户内存，内核内存设计为专用于Linux内核中系统服务使用，是不可swap的，因而内核内存资源对Linux系统来说是非常宝贵的。然而，正因为这种设计，现实中也存在很多针对内核内存资源的攻击，例如恶意进程可以通过不断地fork新进程从而耗尽系统资源，从而造成系统异常或崩溃，这就是所谓的“fork bomb”。

为了防止出现“fork bomb”，社区中就有提议通过linux内核限制cgroup中的kmem容量使用从而限制恶意进程的行为，即[kernel memory accounting机制](https://links.jianshu.com/go?to=https%3A%2F%2Flwn.net%2FArticles%2F516529%2F)。当我们通过memory cgroup限制应用的内存使用时，我们不但需要限制应用对用户内存的使用，也需要限制应用对内核内存的使用。kernel memory accounting机制为cgroup的内存限制增加了stack pages（例如新进程创建）、slab pages(SLAB/SLUB分配器使用的内存)、sockets memory pressure、tcp memory pressure等，[内核文档](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.kernel.org%2Fdoc%2FDocumentation%2Fcgroup-v1%2Fmemory.txt)中有详细的描述。kernel memory accounting机制为cgroup提供了memory.kmem.usage_in_bytes配置项用于限制内核内存的使用，这个配置项与memory.limit_in_bytes（总内存限额）配合使用存在下面三种情形，在[lwn文章](https://links.jianshu.com/go?to=https%3A%2F%2Flwn.net%2FArticles%2F516529%2F)中有详细的介绍：

  a. memory.kmem.limit_in_bytes == unlimited：这种情形不限制内核内存的使用。

  b. memory.kmem.limit_in_bytes < memory.limit_in_bytes：在这种情形下，我们详细的指定了内核内存的上限是多少。

  c. memory.kmem.limit_in_bytes >= memory.limit_in_bytes：在这种情形下，我们只关心包括内核内存及用户内存在内的内存的总使用情况，并不关心内核内存实际使用了多少。在实际使用中，通常是这种情形，我们将memory.kmem.limit_in_bytes设置成大于memory.limit_in_bytes，从而只限制应用的总内存使用。

k8s从1.9版本开始runc默认Enable了kernel memory accounting，即当容器应用设置了memory limit时，容器的memory cgroup中将为memory.kmem.limit_in_bytes设置整形最大值，从而使能内核内存限制机制。我们通过查看memory.kmem.slabinfo的信息即可判断在当前cgroup中kernel memory accounting是否使能，如果未使能，访问这个文件将报输入输出错误。

然而，在4.0以下版本的Linux内核对kernel memory accounting的支持并不完善，社区中逐渐爆出了[slab leak causing a crash when using kmem control group](https://links.jianshu.com/go?to=https%3A%2F%2Fbugzilla.redhat.com%2Fshow_bug.cgi%3Fid%3D1507149)问题，同时runc社区以及Kubernetes社区中也出现了同样的问题，[Enabling kmem accounting can break applications on CentOS7](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fopencontainers%2Frunc%2Fissues%2F1725)、[application crash due to k8s 1.9.x open the kernel memory accounting by default](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fkubernetes%2Fissues%2F61937)。对于这个问题，在内核低于4版本的系统中给docker容器设置kernel-memory限制时将docker将给出以下提示：

> You specified a kernel memory limit on a kernel older than 4.0. Kernel memory limits are experimental on older kernels, it won't work as expected and can cause your system to be unstable.

这个问题也是我们这次遇到问题的根本原因。kmem的泄露导致系统中的服务内存申请失败，从而造成了内核的soft lock。这个问题的解决方法在[Kernel kmem leak caused by newer versions of Docker](https://links.jianshu.com/go?to=https%3A%2F%2Fsupport.mesosphere.com%2Fs%2Farticle%2FCritical-Issue-KMEM-MSPH-2018-0006)这篇文章中有很详细的介绍。我们解决的方案是将docker版本升级，版本特性包括Disable kmem accounting in runc on RHEL/CentOS (docker/escalation#614, docker/escalation#692) docker/engine#121。

---

Linux存储系统包括两个部分：第一部分是站在用户的角度提供读/写的接口，数据以流为表现形式；第二部分是站在存储设备的角度提供读/写接口，数据以块为表现形式。文件系统位于两者中间起到承上启下的作用。

https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram

---

以32位Linux为例，CPU能访问4GB的虚拟地址空间为0x0-0xFFFFFFFF，其中低3GB的地址（0x0-0xC0000000）是应用层的地址空间，高地址的1GB（0xC0000000-0xFFFFFFFF）是留给Kernel的。Kernel中所有的线程共享这1GB的地址空间，而每个进程可以有自己独立的3GB的虚拟地址空间，互不干扰。	

mmap（）将文件（由文件句柄fd所指定）从偏移offset的位置开始的长度为length的一个块映射到内存区域中，从而把文件的某一段映射到进程的地址空间，这样程序就可以通过访问内存的方式去访问文件了。

---

