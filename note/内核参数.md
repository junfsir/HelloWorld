### `vm.max_map_count`

> es启动建议修改的参数；
>
> 此值太小可能会导致Java OOM ERROR，"There is insufficient memory for the Java Runtime Environment to continue"；

```shell
# sysctl -w vm.max_map_count=262144
This file contains the maximum number of memory map areas a process may have. Memory map areas are used as a side-effect of calling malloc, directly by mmap and mprotect, and also when loading shared libraries.

While most applications need less than a thousand maps, certain programs, particularly malloc debuggers, may consume lots of them, e.g., up to one or two maps per allocation.

The default value is 65536.
max_map_count文件包含限制一个进程可以拥有的VMA(虚拟内存区域)的数量。虚拟内存区域是一个连续的虚拟地址空间区域。在进程的生命周期中，每当程序尝试在内存中映射文件，链接到共享内存段，或者分配堆空间的时候，这些区域将被创建。调优这个值将限制进程可拥有VMA的数量。限制一个进程拥有VMA的总数可能导致应用程序出错，因为当进程达到了VMA上线但又只能释放少量的内存给其他的内核进程使用时，操作系统会抛出内存不足的错误。如果你的操作系统在NORMAL区域仅占用少量的内存，那么调低这个值可以帮助释放内存给内核用。
```

[参考](https://www.cnblogs.com/duanxz/p/3567068.html)

### `Transparent Huge Pages`

> hadoop集群中开启内存大页会导致节点CPU SYS高

```shell
# echo never > /sys/kernel/mm/redhat_transparent_hugepage/defrag
# echo never > /sys/kernel/mm/redhat_transparent_hugepage/enabled
# echo never > /sys/kernel/mm/transparent_hugepage/enabled
# echo never > /sys/kernel/mm/transparent_hugepage/defrag
```

### `kernel.pid_max`

```shell
节点全局共享的pid限制，达到预设值后将无法创建进程；
在kubernetes1.14中，支持基于cgroups对pod的pid数量进行限制；
```

```shell
kernel.threads-max
全局可创建的最大线程数
```

### `net.ipv4.tcp_max_syn_backlog`和`net.core.somaxconn`

```shell
# sysctl -w net.ipv4.tcp_max_syn_backlog=102400
# sysctl -w net.core.somaxconn=65535
socket接收的所有连接都是存放在队列类型的数据结构中，关键问题是这种队列有两个，而且其长度都是可以设置的。
分别是下面两个内核参数：
/proc/sys/net/ipv4/tcp_max_syn_backlog
/proc/sys/net/core/somaxconn
其中：
tcp_max_syn_backlog是指定所能接受SYN同步包的最大客户端数量，即半连接上限；
somaxconn是指服务端所能accept即处理数据的最大客户端数量，即完成连接上限；
打个简单的比方：
某某发布公告要邀请四海之内若干客人到场参加酒席。客人参加酒席分为两个步骤：
1、到大厅；
2、找到座位(吃东西，比如糖果、饭菜、酒等)；
tcp_max_syn_backlog用于指定酒席现场面积允许容纳多少人进来；
somaxconn用于指定有多少个座位；
显然tcp_max_syn_backlog>=somaxconn；
如果要前来的客人数量超过tcp_max_syn_backlog，那么多出来的人虽然会跟主任见面握手，但是要在门外等候；
如果到大厅的客人数量大于somaxconn，那么多出来的客人就会没有位置坐(必须坐下才能吃东西)，只能等待有人吃完有空位了才能吃东西；
那么问题来了：
somaxconn是内核里的参数，listen函数有个参数backlog，如果在listen方法里面指定该参数大于somaxconn的值，重新编译并启动程序，服务端所能接收的完整的连接数上限是backlog呢还是somaxconn？
答案很简单，listen方法指定的backlog是在用户态指定的，内核态的参数优先级高于用户态的参数，所以即使在listen方法里面指定backlog是一个大于somaxconn的值，socket在内核态运行时还会检查一次somaxconn，如果连接数超过somaxconn就会等待；
就相当于主人指定了能有多少座位没用，客人到了现场，准备入座时，还要看酒店的客户经理判断能有多少个座位；
```

---

```shell
net.ipv4.tcp_max_syn_backlog = 102400
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_syn_retries = 2
net.ipv4.tcp_fin_timeout = 5
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_max_orphans = 262144
net.ipv4.tcp_mem  = 4194304 6291456 8388608
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65535 16777216
net.ipv4.tcp_no_metrics_save = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_retrans_collapse = 0
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_max_tw_buckets = 30000
net.ipv4.ip_local_port_range = 1024  65535

net.core.rmem_default = 1048576
net.core.netdev_max_backlog = 102400
net.core.rmem_max = 16777216
net.core.wmem_default = 1048576
net.core.wmem_max = 16777216
net.core.somaxconn = 65535

fs.file-max = 2097152 // 所有用户进程所能打开的文件描述符总数

vm.swappiness = 0
vm.max_map_count=282144

net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

net.netfilter.nf_conntrack_max=1048576
net.nf_conntrack_max=1048576
net.netfilter.nf_conntrack_tcp_timeout_fin_wait=30
net.netfilter.nf_conntrack_tcp_timeout_time_wait=30
net.netfilter.nf_conntrack_tcp_timeout_close_wait=15
net.netfilter.nf_conntrack_tcp_timeout_established=300
```

