### `vm.max_map_count`

> es启动建议修改的参数；
>
> 此值太小可能会导致Java OOM ERROR，"There is insufficient memory for the Java Runtime Environment to continue"；

```shell
# sysctl -w vm.max_map_count=262144
This file contains the maximum number of memory map areas a process may have. Memory map areas are used as a side-effect of calling malloc, directly by mmap and mprotect, and also when loading shared libraries.

While most applications need less than a thousand maps, certain programs, particularly malloc debuggers, may consume lots of them, e.g., up to one or two maps per allocation.

The default value is 65536.
max_map_count文件包含限制一个进程可以拥有的VMA(虚拟内存区域)的数量。虚拟内存区域是一个连续的虚拟地址空间区域。在进程的生命周期中，每当程序尝试在内存中映射文件，链接到共享内存段，或者分配堆空间的时候，这些区域将被创建。调优这个值将限制进程可拥有VMA的数量。限制一个进程拥有VMA的总数可能导致应用程序出错，因为当进程达到了VMA上线但又只能释放少量的内存给其他的内核进程使用时，操作系统会抛出内存不足的错误。如果你的操作系统在NORMAL区域仅占用少量的内存，那么调低这个值可以帮助释放内存给内核用。
```

[参考](https://www.cnblogs.com/duanxz/p/3567068.html)

### `Transparent Huge Pages`

> hadoop集群中开启内存大页会导致节点CPU SYS高

```shell
# echo never > /sys/kernel/mm/redhat_transparent_hugepage/defrag
# echo never > /sys/kernel/mm/redhat_transparent_hugepage/enabled
# echo never > /sys/kernel/mm/transparent_hugepage/enabled
# echo never > /sys/kernel/mm/transparent_hugepage/defrag
```

### `kernel.pid_max`

```shell
节点全局共享的pid限制，达到预设值后将无法创建进程；
在kubernetes1.14中，支持基于cgroups对pod的pid数量进行限制；
```

```shell
kernel.threads-max
全局可创建的最大线程数
```

### `net.ipv4.tcp_max_syn_backlog`和`net.core.somaxconn`

```shell
# sysctl -w net.ipv4.tcp_max_syn_backlog=102400
# sysctl -w net.core.somaxconn=65535
socket接收的所有连接都是存放在队列类型的数据结构中，关键问题是这种队列有两个，而且其长度都是可以设置的。
分别是下面两个内核参数：
/proc/sys/net/ipv4/tcp_max_syn_backlog
/proc/sys/net/core/somaxconn
其中：
tcp_max_syn_backlog是指定所能接受SYN同步包的最大客户端数量，即半连接上限；
somaxconn是指服务端所能accept即处理数据的最大客户端数量，即完成连接上限；
打个简单的比方：
某某发布公告要邀请四海之内若干客人到场参加酒席。客人参加酒席分为两个步骤：
1、到大厅；
2、找到座位(吃东西，比如糖果、饭菜、酒等)；
tcp_max_syn_backlog用于指定酒席现场面积允许容纳多少人进来；
somaxconn用于指定有多少个座位；
显然tcp_max_syn_backlog>=somaxconn；
如果要前来的客人数量超过tcp_max_syn_backlog，那么多出来的人虽然会跟主任见面握手，但是要在门外等候；
如果到大厅的客人数量大于somaxconn，那么多出来的客人就会没有位置坐(必须坐下才能吃东西)，只能等待有人吃完有空位了才能吃东西；
那么问题来了：
somaxconn是内核里的参数，listen函数有个参数backlog，如果在listen方法里面指定该参数大于somaxconn的值，重新编译并启动程序，服务端所能接收的完整的连接数上限是backlog呢还是somaxconn？
答案很简单，listen方法指定的backlog是在用户态指定的，内核态的参数优先级高于用户态的参数，所以即使在listen方法里面指定backlog是一个大于somaxconn的值，socket在内核态运行时还会检查一次somaxconn，如果连接数超过somaxconn就会等待；
就相当于主人指定了能有多少座位没用，客人到了现场，准备入座时，还要看酒店的客户经理判断能有多少个座位；
```

---

##  nf_conntrack

`nf_conntrack`是Linux内核连接跟踪的模块，常用在iptables中，比如

```
-A INPUT -m state --state RELATED,ESTABLISHED  -j RETURN
-A INPUT -m state --state INVALID -j DROP
```

可以通过`cat /proc/net/nf_conntrack`来查看当前跟踪的连接信息，这些信息以哈希形式（用链地址法处理冲突）存在内存中，并且每条记录大约占300B空间。

与`nf_conntrack`相关的内核参数有三个：

- `nf_conntrack_max`：连接跟踪表的大小，建议根据内存计算该值`CONNTRACK_MAX = RAMSIZE (in bytes) / 16384 / (x / 32)`，并满足`nf_conntrack_max=4*nf_conntrack_buckets`，默认262144
- `nf_conntrack_buckets`：哈希表的大小，(`nf_conntrack_max/nf_conntrack_buckets`就是每条哈希记录链表的长度)，默认65536
- `nf_conntrack_tcp_timeout_established`：tcp会话的超时时间，默认是432000 (5天)

比如，对64G内存的机器，推荐配置：

```
net.netfilter.nf_conntrack_max=4194304
net.netfilter.nf_conntrack_tcp_timeout_established=300
net.netfilter.nf_conntrack_buckets=1048576
```

---

##  bridge-nf

bridge-nf使得netfilter可以对Linux网桥上的IPv4/ARP/IPv6包过滤。比如，设置`net.bridge.bridge-nf-call-iptables＝1`后，二层的网桥在转发包时也会被iptables的FORWARD规则所过滤，这样有时会出现L3层的iptables rules去过滤L2的帧的问题（见[这里](https://bugzilla.redhat.com/show_bug.cgi?id=512206)）。

常用的选项包括

- net.bridge.bridge-nf-call-arptables：是否在arptables的FORWARD中过滤网桥的ARP包
- net.bridge.bridge-nf-call-ip6tables：是否在ip6tables链中过滤IPv6包
- net.bridge.bridge-nf-call-iptables：是否在iptables链中过滤IPv4包
- net.bridge.bridge-nf-filter-vlan-tagged：是否在iptables/arptables中过滤打了vlan标签的包

当然，也可以通过`/sys/devices/virtual/net/<bridge-name>/bridge/nf_call_iptables`来设置，但要注意内核是取两者中大的生效。

有时，可能只希望部分网桥禁止bridge-nf，而其他网桥都开启（比如CNI网络插件中一般要求bridge-nf-call-iptables选项开启，而有时又希望禁止某个网桥的bridge-nf），这时可以改用iptables的方法：

```sh
iptables -t raw -I PREROUTING -i <bridge-name> -j NOTRACK
```

---

```shell
net.ipv4.tcp_max_syn_backlog = 102400
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_syn_retries = 2
net.ipv4.tcp_fin_timeout = 5
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_max_orphans = 262144
net.ipv4.tcp_mem  = 4194304 6291456 8388608
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65535 16777216
net.ipv4.tcp_no_metrics_save = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_retrans_collapse = 0
net.ipv4.tcp_window_scaling = 1
# 系统同时处理的最大timewait sockets 数目；若超过此值，time-wait socket会被立即砍除并且显示警告信息
net.ipv4.tcp_max_tw_buckets = 30000
net.ipv4.ip_local_port_range = 1024  65535

net.core.rmem_default = 1048576
# 设置当个别接口接收包的速度快于内核处理速度时允许的最大的包序列
net.core.netdev_max_backlog = 102400
net.core.rmem_max = 16777216
net.core.wmem_default = 1048576
net.core.wmem_max = 16777216
# 定义了系统中每一个端口最大的监听队列的长度，这是个全局的参数
net.core.somaxconn = 65535
# 所有用户进程所能打开的文件描述符总数
fs.file-max = 2097152

vm.swappiness = 0
vm.max_map_count=282144

net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

net.netfilter.nf_conntrack_max=1048576
net.nf_conntrack_max=1048576
net.netfilter.nf_conntrack_tcp_timeout_fin_wait=30
net.netfilter.nf_conntrack_tcp_timeout_time_wait=30
net.netfilter.nf_conntrack_tcp_timeout_close_wait=15
net.netfilter.nf_conntrack_tcp_timeout_established=300
```

```shell
# 1：通过反向路径回溯进行源地址验证(在RFC1812中定义)；对于单穴主机和stub网络路
# 由器推荐使用该选项；0：不通过反向路径回溯进行源地址验证
net.ipv4.conf.default.rp_filter=1
# 处理无源路由的包
net.ipv4.conf.default.accept_source_route=0
# Core文件的文件名是否添加应用程序pid做为扩展 0：不添加 1：添加
kernel.core_uses_pid=1
# 开启SYN洪水攻击保护 0：关闭  1：打开
net.ipv4.tcp_syncookies=1
# 单个消息队列中允许的最大字节长度(限制单个消息队列中所有消息包含的字节数之和)
kernel.msgmnb=65536
# 消息队列中单个消息的最大字节数
kernel.msgmax=65536
net.ipv4.conf.all.promote_secondaries=1
net.ipv4.conf.default.promote_secondaries=1
net.ipv6.neigh.default.gc_thresh3=4096
kernel.printk=5
# 系统所允许的最大共享内存段的大小（以字节为单位）
kernel.shmmax=2147483648
net.ipv6.conf.all.disable_ipv6=0
net.ipv6.conf.default.disable_ipv6=0
net.ipv6.conf.lo.disable_ipv6=0
# numa内存平衡
kernel.numa_balancing=0
# 0 (禁用SysRq) 或1 (启用SsyRq) 激活键盘上的sysrq按键。这个按键用于给内核传递信息，用于紧急情况下重启系统。
# 当遇到死机或者没有响应的时候，甚至连 tty 都进不去，可以尝试用 SysRq
# 重启计算机。在终端上同时按Alt, SysRq和命令键则会执行SysRq命令,
# SysRq键就是"Print Screen"健. 比如Alt+SysRq+b则重启机器
kernel.sysrq=1
net.core.rps_sock_flow_entries=8192
net.bridge.bridge-nf-call-ip6tables=1
net.core.rmem_max=16777216
fs.inotify.max_user_watches=99999999
kernel.core_pattern=core
net.core.dev_weight_tx_bias=1
net.ipv4.tcp_max_orphans=65536
kernel.pid_max=4194304
kernel.softlockup_panic=1
fs.file-max=6553600
net.core.bpf_jit_harden=1
net.ipv4.tcp_max_tw_buckets=4096
fs.inotify.max_user_instances=65535
net.core.bpf_jit_kallsyms=1
vm.max_map_count=262144
kernel.threads-max=655350
net.core.bpf_jit_enable=1
net.ipv4.tcp_wmem=4096 87380 8388608
net.core.wmem_max=16777216
net.ipv4.neigh.default.gc_thresh1=2048
net.core.somaxconn=65535
net.ipv4.neigh.default.gc_thresh3=8192
net.ipv4.ip_forward=1
net.ipv4.neigh.default.gc_thresh2=4096
net.ipv4.tcp_max_syn_backlog=65536
net.bridge.bridge-nf-call-iptables=1
net.ipv4.tcp_rmem=4096 87380 8388608
kernel.msgmni=65536
kernel.shmall=524288
kernel.shmmni=4096
kernel.sem=250  32000  100  128
net.ipv4.tcp_keepalive_time=600
net.ipv4.tcp_keepalive_probes=5
net.ipv4.tcp_keepalive_intvl=15
net.ipv4.ip_local_port_range=1024 65000
net.ipv4.tcp_timestamps=1
net.ipv4.tcp_tw_recycle=0 # 新的内核此参数已废弃
net.ipv4.tcp_tw_reuse=1
net.ipv4.tcp_fin_timeout=15
net.core.netdev_max_backlog=2048
net.ipv4.tcp_orphan_retries=3
fs.inotify.max_queued_events=99999999
net.ipv4.ip_local_reserved_ports=60000-61000
net.netfilter.nf_conntrack_max=6553500
net.nf_conntrack_max=6553500
```

https://feisky.gitbooks.io/sdn/content/linux/params.html